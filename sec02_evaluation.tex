
\section{Evaluation of language models}

\subsection{Evaluation metric}

Perplexity (PPL) is the commonly used evaluation metric for language models. 
Perplexity is defined as \cite{PPL}:

\begin{equation}
    PPL(W) = \frac{1}{P(w_1, w_2, \dots, w_N)}
\end{equation}

where $W$ is the test set. Note that a lower perplexity indicates a better model.

Perplexity can also be defined as the exponential of the cross-entropy:

\begin{equation}
    PPL(W) = 2^{H(W)} = 2^{-\frac{1}{N} log_2 P(w_1, w_2, \dots, w_N)}
\end{equation}


\subsection{Dataset}

The experiments have been conducted on the Penn Tree Bank (PTB) dataset 
\cite{PTB}, 
which consists of 929k training words, 73k validation words, and 82k test 
words and a vocabulary composed of 10k words. The material annotated includes 
IBM computer manuals, nursing notes, Wall Street Journal articles, and 
transcribed telephone conversations. The dataset is available from the website 
\textit{deepai.org} \cite{PTB_download}.
