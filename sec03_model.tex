
\begin{figure*}[t]
\centerline{\includegraphics[scale=0.5]{LSTM.PNG}}
\caption{Long-Short Term Memory cell structure and related equations \cite{LSTM_GRU_structure}.}
\label{fig:lstm}
\end{figure*}

\section{Model implementation details}

In this section, we explain some implementation details of the designed 
RNN Language Model. The model is composed of an Embedding layer, a RNN cell 
and a Fully-Connected layer. The model supports two main types of RNN cell: 
LSTM and GRU. A simple version of these cells has been implemented.

The embedding layer is required in order to represent individual words as
real-valued vectors in a predefined embedding space. Each word is mapped 
to one vector and the vector values are learned: words that have 
the same meaning should have a similar representation.

The dropout regularization technique \cite{dropout} has been applied in order
to obtain better generalization performance without overfitting too much the
training set.

A common problem of Recurrent Neural Networks is the \textit{"exploding gradients"} 
due to their instability during training.
A possible easy solution is to apply gradient clipping \cite{clip}: the idea is to
rescale the gradient to keep it small when it gets too large.
This helps gradient descent to have a better behaviour.

\subsection{LSTM cell}


The structure of the Long-Short Term Memory cell is shown in Figure \ref{fig:lstm}
and consists of three main gates:

\begin{enumerate}
    \item The \textit{input gate} decides how much input information is 
    added to the current state and is caracterized by the following Equation:
        \begin{equation}
            i_t = \sigma \left(
                W_i
                \begin{pmatrix}
                x_t \\
                h_{t-1}
                \end{pmatrix}
                + b_i
            \right)
        \end{equation}
    
    \item The \textit{forget gate} decides how much of the past should 
    be remembered and is caracterized by the following Equation:
        \begin{equation}
            f_t = \sigma \left(
                W_f
                \begin{pmatrix}
                x_t \\
                h_{t-1}
                \end{pmatrix}
                + b_f
            \right)
        \end{equation}

    \item The \textit{output gate} decides the output based on the current state and is caracterized by the following Equation:
        \begin{equation}
            o_t = \sigma \left(
                W_o
                \begin{pmatrix}
                x_t \\
                h_{t-1}
                \end{pmatrix}
                + b_o
            \right)
        \end{equation}

\end{enumerate}

% The input gate decides how much input information is added to the current state, the 
% forget gate decides how much of the past should be remembered and the output gate 
% decides the output based on the current state.
% The equations

The memory cell $c_t$ is updated as follows:
\begin{equation}
    c_t = f_t \odot c_{t-1} + i_t \odot tanh \left( W_g
        \begin{pmatrix}
            x_t \\
            h_{t-1}
        \end{pmatrix}
        \right) ;
\end{equation}

and the new hidden state $h_t$ is computed:
\begin{equation}
    h_t = o_t \odot tanh(c_t) .
\end{equation}

The weights $W_i, W_f, W_o, W_g$ are initialized as proposed by Xavier et al. 
\cite{Xavier} and biases $b_i, b_f, b_o$ are initially filled by zero \cite{init}.
Both weights and biases are learned during the training process.





\subsection{GRU cell}

\begin{figure}[hb]
\centerline{\includegraphics[scale=0.25]{GRU_crop.PNG}}
\caption{Gated Recurrent Unit cell structure \cite{LSTM_GRU_structure}.}
\label{fig:gru}
\end{figure}

The structure of the Gated Recurrent Unit is shown in Figure \ref{fig:gru} 
and consists of two main gates:

\begin{enumerate}
    \item The \textit{reset gate} is used to determine how much of the past 
    information to forget and is caracterized by the following Equation:
        \begin{equation}
            r_t = \sigma \left(
                W_r
                \begin{pmatrix}
                x_t \\
                h_{t-1}
                \end{pmatrix}
                + b_r
            \right)
        \end{equation}
    
    \item The \textit{update gate} helps the model to decide how much of the past 
    information needs to be considered and is caracterized by the following Equation:
        \begin{equation}
            z_t = \sigma \left(
                W_z
                \begin{pmatrix}
                x_t \\
                h_{t-1}
                \end{pmatrix}
                + b_z
            \right)
        \end{equation}

\end{enumerate}

Finally, the new hidden state $h_t$ is computed as follows:
\begin{equation}
    h_t' = tanh \left( W
        \begin{pmatrix}
            x_t \\
            r_t \odot h_{t-1}
        \end{pmatrix}
        \right)
\end{equation}

\begin{equation}
    h_t = (1-z_t) \odot h_{t-1} + z_t \odot h_t'
\end{equation}


The weights $W_i, W_f, W_o, W_g$ are initialized as proposed by Xavier et al. 
\cite{Xavier} and biases $b_i, b_f, b_o$ are initially filled by zero.
Both weights and biases are learned during the training process.
